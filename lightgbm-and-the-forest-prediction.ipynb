{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/learn-together/sample_submission.csv\n",
      "/kaggle/input/learn-together/test.csv\n",
      "/kaggle/input/learn-together/train.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "# Read the data and save it in a dataframe\n",
    "\n",
    "# Use r when data is stored locally on your haard drive\n",
    "##X_train = pd.read_csv(r'C:\\Users\\Murphy\\Desktop\\Kaggle Forestry\\learn-together\\train.csv', index_col = 'Id')\n",
    "##X_test  = pd.read_csv(r'C:\\Users\\Murphy\\Desktop\\Kaggle Forestry\\learn-together\\test.csv',  index_col = 'Id')\n",
    "##X_test_full  = pd.read_csv(r'C:\\Users\\Murphy\\Desktop\\Kaggle Forestry\\learn-together\\test.csv')\n",
    "X_train = pd.read_csv(\"/kaggle/input/learn-together/train.csv\", index_col = 'Id')\n",
    "X_test = pd.read_csv(\"/kaggle/input/learn-together/test.csv\", index_col = 'Id')\n",
    "X_test_full = pd.read_csv(\"/kaggle/input/learn-together/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create target object and call it y\n",
    "y = X_train.Cover_Type\n",
    "\n",
    "# Create feature object and call it X. Drop what we are trying to predict: Cover_Type\n",
    "X = X_train.drop(columns = 'Cover_Type')\n",
    "\n",
    "# Split into test and training data\n",
    "X_train_full, X_test_full, y_train, y_test = train_test_split(X, y, train_size=0.8, test_size=0.2,random_state=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list features_to_try is used so we can drop some features and test the model without various features.\n",
    "To_Do: impute the soil features to one column..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology',\n",
      "       'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways',\n",
      "       'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm',\n",
      "       'Horizontal_Distance_To_Fire_Points', 'Wilderness_Area1',\n",
      "       'Wilderness_Area2', 'Wilderness_Area3', 'Wilderness_Area4',\n",
      "       'Soil_Type1', 'Soil_Type2', 'Soil_Type3', 'Soil_Type4', 'Soil_Type5',\n",
      "       'Soil_Type6', 'Soil_Type7', 'Soil_Type8', 'Soil_Type9', 'Soil_Type10',\n",
      "       'Soil_Type11', 'Soil_Type12', 'Soil_Type13', 'Soil_Type14',\n",
      "       'Soil_Type15', 'Soil_Type16', 'Soil_Type17', 'Soil_Type18',\n",
      "       'Soil_Type19', 'Soil_Type20', 'Soil_Type21', 'Soil_Type22',\n",
      "       'Soil_Type23', 'Soil_Type24', 'Soil_Type25', 'Soil_Type26',\n",
      "       'Soil_Type27', 'Soil_Type28', 'Soil_Type29', 'Soil_Type30',\n",
      "       'Soil_Type31', 'Soil_Type32', 'Soil_Type33', 'Soil_Type34',\n",
      "       'Soil_Type35', 'Soil_Type36', 'Soil_Type37', 'Soil_Type38',\n",
      "       'Soil_Type39', 'Soil_Type40', 'Cover_Type'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "feature_titles = X_train.columns\n",
    "print(feature_titles)\n",
    "\n",
    "##Take out Soil_Type: 7,8,9,10,15,16,25\n",
    "features_to_try = ['Elevation', 'Aspect', 'Slope', \n",
    "       'Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology',\n",
    "       'Horizontal_Distance_To_Roadways', 'Hillshade_9am', 'Hillshade_Noon',\n",
    "       'Hillshade_3pm', 'Horizontal_Distance_To_Fire_Points',\n",
    "        'Wilderness_Area1', 'Wilderness_Area2', 'Wilderness_Area3',\n",
    "       'Wilderness_Area4', 'Soil_Type1', 'Soil_Type2', 'Soil_Type3',\n",
    "       'Soil_Type4', 'Soil_Type5', 'Soil_Type6', \n",
    "       'Soil_Type11', 'Soil_Type12',\n",
    "       'Soil_Type13', 'Soil_Type14', 'Soil_Type15', 'Soil_Type16',\n",
    "       'Soil_Type17', 'Soil_Type18', 'Soil_Type19', 'Soil_Type20',\n",
    "       'Soil_Type21', 'Soil_Type22', 'Soil_Type23', 'Soil_Type24',\n",
    "       'Soil_Type25', 'Soil_Type26', 'Soil_Type27', 'Soil_Type28',\n",
    "       'Soil_Type29', 'Soil_Type30', 'Soil_Type31', 'Soil_Type32',\n",
    "       'Soil_Type33', 'Soil_Type34', 'Soil_Type35', 'Soil_Type36',\n",
    "       'Soil_Type37', 'Soil_Type38', 'Soil_Type39', 'Soil_Type40']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Id\n",
       "6909     2\n",
       "3112     1\n",
       "8911     5\n",
       "6100     4\n",
       "4884     4\n",
       "        ..\n",
       "906      5\n",
       "5193     6\n",
       "12173    6\n",
       "236      2\n",
       "13350    6\n",
       "Name: Cover_Type, Length: 12096, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology',\n",
       "       'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways',\n",
       "       'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm',\n",
       "       'Horizontal_Distance_To_Fire_Points', 'Wilderness_Area1',\n",
       "       'Wilderness_Area2', 'Wilderness_Area3', 'Wilderness_Area4',\n",
       "       'Soil_Type1', 'Soil_Type2', 'Soil_Type3', 'Soil_Type4', 'Soil_Type5',\n",
       "       'Soil_Type6', 'Soil_Type11', 'Soil_Type12', 'Soil_Type13',\n",
       "       'Soil_Type14', 'Soil_Type15', 'Soil_Type16', 'Soil_Type17',\n",
       "       'Soil_Type18', 'Soil_Type19', 'Soil_Type20', 'Soil_Type21',\n",
       "       'Soil_Type22', 'Soil_Type23', 'Soil_Type24', 'Soil_Type25',\n",
       "       'Soil_Type26', 'Soil_Type27', 'Soil_Type28', 'Soil_Type29',\n",
       "       'Soil_Type30', 'Soil_Type31', 'Soil_Type32', 'Soil_Type33',\n",
       "       'Soil_Type34', 'Soil_Type35', 'Soil_Type36', 'Soil_Type37',\n",
       "       'Soil_Type38', 'Soil_Type39', 'Soil_Type40'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#set up new X_train and X_test with the reduced features.\n",
    "X_train_feature_reduction = X_train_full[features_to_try]\n",
    "X_test_feature_reduction  = X_test_full[features_to_try]\n",
    "X_train_feature_reduction.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\ttraining's multi_logloss: 1.85809\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[2]\ttraining's multi_logloss: 1.78212\n",
      "[3]\ttraining's multi_logloss: 1.71564\n",
      "[4]\ttraining's multi_logloss: 1.65938\n",
      "[5]\ttraining's multi_logloss: 1.59083\n",
      "[6]\ttraining's multi_logloss: 1.53083\n",
      "[7]\ttraining's multi_logloss: 1.48688\n",
      "[8]\ttraining's multi_logloss: 1.43749\n",
      "[9]\ttraining's multi_logloss: 1.39197\n",
      "[10]\ttraining's multi_logloss: 1.34913\n",
      "[11]\ttraining's multi_logloss: 1.30389\n",
      "[12]\ttraining's multi_logloss: 1.2612\n",
      "[13]\ttraining's multi_logloss: 1.22783\n",
      "[14]\ttraining's multi_logloss: 1.18963\n",
      "[15]\ttraining's multi_logloss: 1.15606\n",
      "[16]\ttraining's multi_logloss: 1.12285\n",
      "[17]\ttraining's multi_logloss: 1.08993\n",
      "[18]\ttraining's multi_logloss: 1.06105\n",
      "[19]\ttraining's multi_logloss: 1.03269\n",
      "[20]\ttraining's multi_logloss: 1.00513\n",
      "[21]\ttraining's multi_logloss: 0.978827\n",
      "[22]\ttraining's multi_logloss: 0.953152\n",
      "[23]\ttraining's multi_logloss: 0.927999\n",
      "[24]\ttraining's multi_logloss: 0.904406\n",
      "[25]\ttraining's multi_logloss: 0.88184\n",
      "[26]\ttraining's multi_logloss: 0.864005\n",
      "[27]\ttraining's multi_logloss: 0.843579\n",
      "[28]\ttraining's multi_logloss: 0.826046\n",
      "[29]\ttraining's multi_logloss: 0.809336\n",
      "[30]\ttraining's multi_logloss: 0.793375\n",
      "[31]\ttraining's multi_logloss: 0.776198\n",
      "[32]\ttraining's multi_logloss: 0.760373\n",
      "[33]\ttraining's multi_logloss: 0.743824\n",
      "[34]\ttraining's multi_logloss: 0.728568\n",
      "[35]\ttraining's multi_logloss: 0.713341\n",
      "[36]\ttraining's multi_logloss: 0.69781\n",
      "[37]\ttraining's multi_logloss: 0.68359\n",
      "[38]\ttraining's multi_logloss: 0.669762\n",
      "[39]\ttraining's multi_logloss: 0.656854\n",
      "[40]\ttraining's multi_logloss: 0.644586\n",
      "[41]\ttraining's multi_logloss: 0.632182\n",
      "[42]\ttraining's multi_logloss: 0.619798\n",
      "[43]\ttraining's multi_logloss: 0.608042\n",
      "[44]\ttraining's multi_logloss: 0.59585\n",
      "[45]\ttraining's multi_logloss: 0.585599\n",
      "[46]\ttraining's multi_logloss: 0.575859\n",
      "[47]\ttraining's multi_logloss: 0.564723\n",
      "[48]\ttraining's multi_logloss: 0.555003\n",
      "[49]\ttraining's multi_logloss: 0.545308\n",
      "[50]\ttraining's multi_logloss: 0.536378\n",
      "[51]\ttraining's multi_logloss: 0.526712\n",
      "[52]\ttraining's multi_logloss: 0.518043\n",
      "[53]\ttraining's multi_logloss: 0.509237\n",
      "[54]\ttraining's multi_logloss: 0.499979\n",
      "[55]\ttraining's multi_logloss: 0.491875\n",
      "[56]\ttraining's multi_logloss: 0.484311\n",
      "[57]\ttraining's multi_logloss: 0.476506\n",
      "[58]\ttraining's multi_logloss: 0.469116\n",
      "[59]\ttraining's multi_logloss: 0.461461\n",
      "[60]\ttraining's multi_logloss: 0.454614\n",
      "[61]\ttraining's multi_logloss: 0.447198\n",
      "[62]\ttraining's multi_logloss: 0.440857\n",
      "[63]\ttraining's multi_logloss: 0.434491\n",
      "[64]\ttraining's multi_logloss: 0.428261\n",
      "[65]\ttraining's multi_logloss: 0.421711\n",
      "[66]\ttraining's multi_logloss: 0.415193\n",
      "[67]\ttraining's multi_logloss: 0.408968\n",
      "[68]\ttraining's multi_logloss: 0.402429\n",
      "[69]\ttraining's multi_logloss: 0.397125\n",
      "[70]\ttraining's multi_logloss: 0.391839\n",
      "[71]\ttraining's multi_logloss: 0.386644\n",
      "[72]\ttraining's multi_logloss: 0.381086\n",
      "[73]\ttraining's multi_logloss: 0.376249\n",
      "[74]\ttraining's multi_logloss: 0.3708\n",
      "[75]\ttraining's multi_logloss: 0.36606\n",
      "[76]\ttraining's multi_logloss: 0.361516\n",
      "[77]\ttraining's multi_logloss: 0.356269\n",
      "[78]\ttraining's multi_logloss: 0.351546\n",
      "[79]\ttraining's multi_logloss: 0.347018\n",
      "[80]\ttraining's multi_logloss: 0.34272\n",
      "[81]\ttraining's multi_logloss: 0.33805\n",
      "[82]\ttraining's multi_logloss: 0.333617\n",
      "[83]\ttraining's multi_logloss: 0.329077\n",
      "[84]\ttraining's multi_logloss: 0.324822\n",
      "[85]\ttraining's multi_logloss: 0.319885\n",
      "[86]\ttraining's multi_logloss: 0.31536\n",
      "[87]\ttraining's multi_logloss: 0.311152\n",
      "[88]\ttraining's multi_logloss: 0.307463\n",
      "[89]\ttraining's multi_logloss: 0.303619\n",
      "[90]\ttraining's multi_logloss: 0.299591\n",
      "[91]\ttraining's multi_logloss: 0.295886\n",
      "[92]\ttraining's multi_logloss: 0.292018\n",
      "[93]\ttraining's multi_logloss: 0.288438\n",
      "[94]\ttraining's multi_logloss: 0.284911\n",
      "[95]\ttraining's multi_logloss: 0.28184\n",
      "[96]\ttraining's multi_logloss: 0.279002\n",
      "[97]\ttraining's multi_logloss: 0.275962\n",
      "[98]\ttraining's multi_logloss: 0.272867\n",
      "[99]\ttraining's multi_logloss: 0.269676\n",
      "[100]\ttraining's multi_logloss: 0.266647\n",
      "[101]\ttraining's multi_logloss: 0.263311\n",
      "[102]\ttraining's multi_logloss: 0.260352\n",
      "[103]\ttraining's multi_logloss: 0.256897\n",
      "[104]\ttraining's multi_logloss: 0.253759\n",
      "[105]\ttraining's multi_logloss: 0.250865\n",
      "[106]\ttraining's multi_logloss: 0.247843\n",
      "[107]\ttraining's multi_logloss: 0.244709\n",
      "[108]\ttraining's multi_logloss: 0.241523\n",
      "[109]\ttraining's multi_logloss: 0.238704\n",
      "[110]\ttraining's multi_logloss: 0.235642\n",
      "[111]\ttraining's multi_logloss: 0.232622\n",
      "[112]\ttraining's multi_logloss: 0.229991\n",
      "[113]\ttraining's multi_logloss: 0.227552\n",
      "[114]\ttraining's multi_logloss: 0.224967\n",
      "[115]\ttraining's multi_logloss: 0.222673\n",
      "[116]\ttraining's multi_logloss: 0.220383\n",
      "[117]\ttraining's multi_logloss: 0.217934\n",
      "[118]\ttraining's multi_logloss: 0.215394\n",
      "[119]\ttraining's multi_logloss: 0.213358\n",
      "[120]\ttraining's multi_logloss: 0.211046\n",
      "[121]\ttraining's multi_logloss: 0.208347\n",
      "[122]\ttraining's multi_logloss: 0.205559\n",
      "[123]\ttraining's multi_logloss: 0.203347\n",
      "[124]\ttraining's multi_logloss: 0.201058\n",
      "[125]\ttraining's multi_logloss: 0.198849\n",
      "[126]\ttraining's multi_logloss: 0.196484\n",
      "[127]\ttraining's multi_logloss: 0.194224\n",
      "[128]\ttraining's multi_logloss: 0.191959\n",
      "[129]\ttraining's multi_logloss: 0.189668\n",
      "[130]\ttraining's multi_logloss: 0.187474\n",
      "[131]\ttraining's multi_logloss: 0.185574\n",
      "[132]\ttraining's multi_logloss: 0.183742\n",
      "[133]\ttraining's multi_logloss: 0.181783\n",
      "[134]\ttraining's multi_logloss: 0.179976\n",
      "[135]\ttraining's multi_logloss: 0.178215\n",
      "[136]\ttraining's multi_logloss: 0.176388\n",
      "[137]\ttraining's multi_logloss: 0.174956\n",
      "[138]\ttraining's multi_logloss: 0.173279\n",
      "[139]\ttraining's multi_logloss: 0.171525\n",
      "[140]\ttraining's multi_logloss: 0.169948\n",
      "[141]\ttraining's multi_logloss: 0.167854\n",
      "[142]\ttraining's multi_logloss: 0.165873\n",
      "[143]\ttraining's multi_logloss: 0.164081\n",
      "[144]\ttraining's multi_logloss: 0.162361\n",
      "[145]\ttraining's multi_logloss: 0.160466\n",
      "[146]\ttraining's multi_logloss: 0.158684\n",
      "[147]\ttraining's multi_logloss: 0.156937\n",
      "[148]\ttraining's multi_logloss: 0.155368\n",
      "[149]\ttraining's multi_logloss: 0.153891\n",
      "[150]\ttraining's multi_logloss: 0.152314\n",
      "[151]\ttraining's multi_logloss: 0.150683\n",
      "[152]\ttraining's multi_logloss: 0.149268\n",
      "[153]\ttraining's multi_logloss: 0.147945\n",
      "[154]\ttraining's multi_logloss: 0.146617\n",
      "[155]\ttraining's multi_logloss: 0.14531\n",
      "[156]\ttraining's multi_logloss: 0.143969\n",
      "[157]\ttraining's multi_logloss: 0.142739\n",
      "[158]\ttraining's multi_logloss: 0.141501\n",
      "[159]\ttraining's multi_logloss: 0.140321\n",
      "[160]\ttraining's multi_logloss: 0.1391\n",
      "[161]\ttraining's multi_logloss: 0.137388\n",
      "[162]\ttraining's multi_logloss: 0.135843\n",
      "[163]\ttraining's multi_logloss: 0.134166\n",
      "[164]\ttraining's multi_logloss: 0.132666\n",
      "[165]\ttraining's multi_logloss: 0.13107\n",
      "[166]\ttraining's multi_logloss: 0.129753\n",
      "[167]\ttraining's multi_logloss: 0.128294\n",
      "[168]\ttraining's multi_logloss: 0.12687\n",
      "[169]\ttraining's multi_logloss: 0.125343\n",
      "[170]\ttraining's multi_logloss: 0.124118\n",
      "[171]\ttraining's multi_logloss: 0.122955\n",
      "[172]\ttraining's multi_logloss: 0.121885\n",
      "[173]\ttraining's multi_logloss: 0.120734\n",
      "[174]\ttraining's multi_logloss: 0.119504\n",
      "[175]\ttraining's multi_logloss: 0.118447\n",
      "[176]\ttraining's multi_logloss: 0.117307\n",
      "[177]\ttraining's multi_logloss: 0.116306\n",
      "[178]\ttraining's multi_logloss: 0.115259\n",
      "[179]\ttraining's multi_logloss: 0.114209\n",
      "[180]\ttraining's multi_logloss: 0.113187\n",
      "[181]\ttraining's multi_logloss: 0.111747\n",
      "[182]\ttraining's multi_logloss: 0.110432\n",
      "[183]\ttraining's multi_logloss: 0.109076\n",
      "[184]\ttraining's multi_logloss: 0.107836\n",
      "[185]\ttraining's multi_logloss: 0.106681\n",
      "[186]\ttraining's multi_logloss: 0.105548\n",
      "[187]\ttraining's multi_logloss: 0.104429\n",
      "[188]\ttraining's multi_logloss: 0.103427\n",
      "[189]\ttraining's multi_logloss: 0.102377\n",
      "[190]\ttraining's multi_logloss: 0.101355\n",
      "[191]\ttraining's multi_logloss: 0.100244\n",
      "[192]\ttraining's multi_logloss: 0.0993083\n",
      "[193]\ttraining's multi_logloss: 0.0982746\n",
      "[194]\ttraining's multi_logloss: 0.0973474\n",
      "[195]\ttraining's multi_logloss: 0.0964767\n",
      "[196]\ttraining's multi_logloss: 0.0955207\n",
      "[197]\ttraining's multi_logloss: 0.0947505\n",
      "[198]\ttraining's multi_logloss: 0.0940591\n",
      "[199]\ttraining's multi_logloss: 0.0932949\n",
      "[200]\ttraining's multi_logloss: 0.092535\n",
      "[201]\ttraining's multi_logloss: 0.0914568\n",
      "[202]\ttraining's multi_logloss: 0.090352\n",
      "[203]\ttraining's multi_logloss: 0.0893965\n",
      "[204]\ttraining's multi_logloss: 0.0883971\n",
      "[205]\ttraining's multi_logloss: 0.0874544\n",
      "[206]\ttraining's multi_logloss: 0.0865973\n",
      "[207]\ttraining's multi_logloss: 0.0857887\n",
      "[208]\ttraining's multi_logloss: 0.0849456\n",
      "[209]\ttraining's multi_logloss: 0.08416\n",
      "[210]\ttraining's multi_logloss: 0.0833757\n",
      "[211]\ttraining's multi_logloss: 0.0826814\n",
      "[212]\ttraining's multi_logloss: 0.0819268\n",
      "[213]\ttraining's multi_logloss: 0.0811982\n",
      "[214]\ttraining's multi_logloss: 0.0805569\n",
      "[215]\ttraining's multi_logloss: 0.0799095\n",
      "[216]\ttraining's multi_logloss: 0.0793116\n",
      "[217]\ttraining's multi_logloss: 0.0787072\n",
      "[218]\ttraining's multi_logloss: 0.0781315\n",
      "[219]\ttraining's multi_logloss: 0.0775155\n",
      "[220]\ttraining's multi_logloss: 0.0769594\n",
      "[221]\ttraining's multi_logloss: 0.0760856\n",
      "[222]\ttraining's multi_logloss: 0.0751774\n",
      "[223]\ttraining's multi_logloss: 0.0742985\n",
      "[224]\ttraining's multi_logloss: 0.073458\n",
      "[225]\ttraining's multi_logloss: 0.0726857\n",
      "[226]\ttraining's multi_logloss: 0.0719144\n",
      "[227]\ttraining's multi_logloss: 0.0711722\n",
      "[228]\ttraining's multi_logloss: 0.0704535\n",
      "[229]\ttraining's multi_logloss: 0.0698091\n",
      "[230]\ttraining's multi_logloss: 0.0691548\n",
      "[231]\ttraining's multi_logloss: 0.0685065\n",
      "[232]\ttraining's multi_logloss: 0.0679325\n",
      "[233]\ttraining's multi_logloss: 0.0673589\n",
      "[234]\ttraining's multi_logloss: 0.0668374\n",
      "[235]\ttraining's multi_logloss: 0.0663091\n",
      "[236]\ttraining's multi_logloss: 0.0657983\n",
      "[237]\ttraining's multi_logloss: 0.0652675\n",
      "[238]\ttraining's multi_logloss: 0.0647592\n",
      "[239]\ttraining's multi_logloss: 0.0642973\n",
      "[240]\ttraining's multi_logloss: 0.0638525\n",
      "[241]\ttraining's multi_logloss: 0.0631057\n",
      "[242]\ttraining's multi_logloss: 0.0624498\n",
      "[243]\ttraining's multi_logloss: 0.061747\n",
      "[244]\ttraining's multi_logloss: 0.0610575\n",
      "[245]\ttraining's multi_logloss: 0.0604437\n",
      "[246]\ttraining's multi_logloss: 0.0598447\n",
      "[247]\ttraining's multi_logloss: 0.0593158\n",
      "[248]\ttraining's multi_logloss: 0.0587962\n",
      "[249]\ttraining's multi_logloss: 0.0582555\n",
      "[250]\ttraining's multi_logloss: 0.0577598\n",
      "[251]\ttraining's multi_logloss: 0.0572303\n",
      "[252]\ttraining's multi_logloss: 0.0567846\n",
      "[253]\ttraining's multi_logloss: 0.0562746\n",
      "[254]\ttraining's multi_logloss: 0.0557995\n",
      "[255]\ttraining's multi_logloss: 0.0554053\n",
      "[256]\ttraining's multi_logloss: 0.0550123\n",
      "[257]\ttraining's multi_logloss: 0.0546226\n",
      "[258]\ttraining's multi_logloss: 0.0542494\n",
      "[259]\ttraining's multi_logloss: 0.0538997\n",
      "[260]\ttraining's multi_logloss: 0.0535413\n",
      "[261]\ttraining's multi_logloss: 0.0529147\n",
      "[262]\ttraining's multi_logloss: 0.0523323\n",
      "[263]\ttraining's multi_logloss: 0.0517476\n",
      "[264]\ttraining's multi_logloss: 0.0512159\n",
      "[265]\ttraining's multi_logloss: 0.0507073\n",
      "[266]\ttraining's multi_logloss: 0.0502405\n",
      "[267]\ttraining's multi_logloss: 0.0497105\n",
      "[268]\ttraining's multi_logloss: 0.0492163\n",
      "[269]\ttraining's multi_logloss: 0.0487805\n",
      "[270]\ttraining's multi_logloss: 0.0483352\n",
      "[271]\ttraining's multi_logloss: 0.0479382\n",
      "[272]\ttraining's multi_logloss: 0.0474937\n",
      "[273]\ttraining's multi_logloss: 0.0471168\n",
      "[274]\ttraining's multi_logloss: 0.0467148\n",
      "[275]\ttraining's multi_logloss: 0.0463591\n",
      "[276]\ttraining's multi_logloss: 0.0460293\n",
      "[277]\ttraining's multi_logloss: 0.0457384\n",
      "[278]\ttraining's multi_logloss: 0.0454164\n",
      "[279]\ttraining's multi_logloss: 0.0451197\n",
      "[280]\ttraining's multi_logloss: 0.0448089\n",
      "[281]\ttraining's multi_logloss: 0.0441576\n",
      "[282]\ttraining's multi_logloss: 0.0436255\n",
      "[283]\ttraining's multi_logloss: 0.0431439\n",
      "[284]\ttraining's multi_logloss: 0.0426568\n",
      "[285]\ttraining's multi_logloss: 0.0421854\n",
      "[286]\ttraining's multi_logloss: 0.041792\n",
      "[287]\ttraining's multi_logloss: 0.0413698\n",
      "[288]\ttraining's multi_logloss: 0.0409651\n",
      "[289]\ttraining's multi_logloss: 0.0406051\n",
      "[290]\ttraining's multi_logloss: 0.0402663\n",
      "[291]\ttraining's multi_logloss: 0.0399275\n",
      "[292]\ttraining's multi_logloss: 0.0396041\n",
      "[293]\ttraining's multi_logloss: 0.0392913\n",
      "[294]\ttraining's multi_logloss: 0.0389859\n",
      "[295]\ttraining's multi_logloss: 0.0387147\n",
      "[296]\ttraining's multi_logloss: 0.0384463\n",
      "[297]\ttraining's multi_logloss: 0.0381789\n",
      "[298]\ttraining's multi_logloss: 0.0378974\n",
      "[299]\ttraining's multi_logloss: 0.0376617\n",
      "[300]\ttraining's multi_logloss: 0.0374239\n",
      "[301]\ttraining's multi_logloss: 0.0370363\n",
      "[302]\ttraining's multi_logloss: 0.0366441\n",
      "[303]\ttraining's multi_logloss: 0.0362833\n",
      "[304]\ttraining's multi_logloss: 0.0359482\n",
      "[305]\ttraining's multi_logloss: 0.0355984\n",
      "[306]\ttraining's multi_logloss: 0.0352737\n",
      "[307]\ttraining's multi_logloss: 0.0349336\n",
      "[308]\ttraining's multi_logloss: 0.034664\n",
      "[309]\ttraining's multi_logloss: 0.0343892\n",
      "[310]\ttraining's multi_logloss: 0.0340909\n",
      "[311]\ttraining's multi_logloss: 0.0338653\n",
      "[312]\ttraining's multi_logloss: 0.0336376\n",
      "[313]\ttraining's multi_logloss: 0.0334433\n",
      "[314]\ttraining's multi_logloss: 0.0332201\n",
      "[315]\ttraining's multi_logloss: 0.0330051\n",
      "[316]\ttraining's multi_logloss: 0.0327637\n",
      "[317]\ttraining's multi_logloss: 0.0325588\n",
      "[318]\ttraining's multi_logloss: 0.0323578\n",
      "[319]\ttraining's multi_logloss: 0.0322106\n",
      "[320]\ttraining's multi_logloss: 0.0320502\n",
      "[321]\ttraining's multi_logloss: 0.0314303\n",
      "[322]\ttraining's multi_logloss: 0.0310628\n",
      "[323]\ttraining's multi_logloss: 0.0307213\n",
      "[324]\ttraining's multi_logloss: 0.0303675\n",
      "[325]\ttraining's multi_logloss: 0.0300706\n",
      "[326]\ttraining's multi_logloss: 0.0297953\n",
      "[327]\ttraining's multi_logloss: 0.0295202\n",
      "[328]\ttraining's multi_logloss: 0.0292819\n",
      "[329]\ttraining's multi_logloss: 0.029004\n",
      "[330]\ttraining's multi_logloss: 0.0287836\n",
      "[331]\ttraining's multi_logloss: 0.0285873\n",
      "[332]\ttraining's multi_logloss: 0.0283835\n",
      "[333]\ttraining's multi_logloss: 0.0281629\n",
      "[334]\ttraining's multi_logloss: 0.0279801\n",
      "[335]\ttraining's multi_logloss: 0.0278022\n",
      "[336]\ttraining's multi_logloss: 0.0276248\n",
      "[337]\ttraining's multi_logloss: 0.0274377\n",
      "[338]\ttraining's multi_logloss: 0.0272451\n",
      "[339]\ttraining's multi_logloss: 0.0271054\n",
      "[340]\ttraining's multi_logloss: 0.0269566\n",
      "[341]\ttraining's multi_logloss: 0.0266649\n",
      "[342]\ttraining's multi_logloss: 0.0264015\n",
      "[343]\ttraining's multi_logloss: 0.0261573\n",
      "[344]\ttraining's multi_logloss: 0.0259082\n",
      "[345]\ttraining's multi_logloss: 0.0256851\n",
      "[346]\ttraining's multi_logloss: 0.0254651\n",
      "[347]\ttraining's multi_logloss: 0.025281\n",
      "[348]\ttraining's multi_logloss: 0.0250692\n",
      "[349]\ttraining's multi_logloss: 0.0248477\n",
      "[350]\ttraining's multi_logloss: 0.0247003\n",
      "[351]\ttraining's multi_logloss: 0.024542\n",
      "[352]\ttraining's multi_logloss: 0.0243533\n",
      "[353]\ttraining's multi_logloss: 0.0241763\n",
      "[354]\ttraining's multi_logloss: 0.0240063\n",
      "[355]\ttraining's multi_logloss: 0.0238127\n",
      "[356]\ttraining's multi_logloss: 0.0236522\n",
      "[357]\ttraining's multi_logloss: 0.0235042\n",
      "[358]\ttraining's multi_logloss: 0.0233703\n",
      "[359]\ttraining's multi_logloss: 0.0232389\n",
      "[360]\ttraining's multi_logloss: 0.0231002\n",
      "[361]\ttraining's multi_logloss: 0.02286\n",
      "[362]\ttraining's multi_logloss: 0.0225875\n",
      "[363]\ttraining's multi_logloss: 0.0222703\n",
      "[364]\ttraining's multi_logloss: 0.022032\n",
      "[365]\ttraining's multi_logloss: 0.021812\n",
      "[366]\ttraining's multi_logloss: 0.0216205\n",
      "[367]\ttraining's multi_logloss: 0.0214248\n",
      "[368]\ttraining's multi_logloss: 0.021254\n",
      "[369]\ttraining's multi_logloss: 0.0210703\n",
      "[370]\ttraining's multi_logloss: 0.0209211\n",
      "[371]\ttraining's multi_logloss: 0.0207621\n",
      "[372]\ttraining's multi_logloss: 0.0205996\n",
      "[373]\ttraining's multi_logloss: 0.0204788\n",
      "[374]\ttraining's multi_logloss: 0.0203642\n",
      "[375]\ttraining's multi_logloss: 0.0202423\n",
      "[376]\ttraining's multi_logloss: 0.0201389\n",
      "[377]\ttraining's multi_logloss: 0.0200343\n",
      "[378]\ttraining's multi_logloss: 0.0199253\n",
      "[379]\ttraining's multi_logloss: 0.0198203\n",
      "[380]\ttraining's multi_logloss: 0.0197122\n",
      "[381]\ttraining's multi_logloss: 0.0194259\n",
      "[382]\ttraining's multi_logloss: 0.0191135\n",
      "[383]\ttraining's multi_logloss: 0.0188657\n",
      "[384]\ttraining's multi_logloss: 0.0186021\n",
      "[385]\ttraining's multi_logloss: 0.0182862\n",
      "[386]\ttraining's multi_logloss: 0.0180748\n",
      "[387]\ttraining's multi_logloss: 0.0178481\n",
      "[388]\ttraining's multi_logloss: 0.0176716\n",
      "[389]\ttraining's multi_logloss: 0.017497\n",
      "[390]\ttraining's multi_logloss: 0.0173513\n",
      "[391]\ttraining's multi_logloss: 0.0172063\n",
      "[392]\ttraining's multi_logloss: 0.0170365\n",
      "[393]\ttraining's multi_logloss: 0.016883\n",
      "[394]\ttraining's multi_logloss: 0.0167348\n",
      "[395]\ttraining's multi_logloss: 0.0166102\n",
      "[396]\ttraining's multi_logloss: 0.0164692\n",
      "[397]\ttraining's multi_logloss: 0.0163595\n",
      "[398]\ttraining's multi_logloss: 0.0162546\n",
      "[399]\ttraining's multi_logloss: 0.0161702\n",
      "[400]\ttraining's multi_logloss: 0.0160957\n",
      "[401]\ttraining's multi_logloss: 0.0159292\n",
      "[402]\ttraining's multi_logloss: 0.0157731\n",
      "[403]\ttraining's multi_logloss: 0.0156217\n",
      "[404]\ttraining's multi_logloss: 0.0154797\n",
      "[405]\ttraining's multi_logloss: 0.0153476\n",
      "[406]\ttraining's multi_logloss: 0.0152378\n",
      "[407]\ttraining's multi_logloss: 0.0151251\n",
      "[408]\ttraining's multi_logloss: 0.0150022\n",
      "[409]\ttraining's multi_logloss: 0.0149001\n",
      "[410]\ttraining's multi_logloss: 0.0147766\n",
      "[411]\ttraining's multi_logloss: 0.0146756\n",
      "[412]\ttraining's multi_logloss: 0.0145777\n",
      "[413]\ttraining's multi_logloss: 0.0144681\n",
      "[414]\ttraining's multi_logloss: 0.0143677\n",
      "[415]\ttraining's multi_logloss: 0.0142785\n",
      "[416]\ttraining's multi_logloss: 0.0142138\n",
      "[417]\ttraining's multi_logloss: 0.0141375\n",
      "[418]\ttraining's multi_logloss: 0.0140709\n",
      "[419]\ttraining's multi_logloss: 0.0139991\n",
      "[420]\ttraining's multi_logloss: 0.0139178\n",
      "[421]\ttraining's multi_logloss: 0.0137993\n",
      "[422]\ttraining's multi_logloss: 0.0136829\n",
      "[423]\ttraining's multi_logloss: 0.0135686\n",
      "[424]\ttraining's multi_logloss: 0.0134466\n",
      "[425]\ttraining's multi_logloss: 0.0133525\n",
      "[426]\ttraining's multi_logloss: 0.013249\n",
      "[427]\ttraining's multi_logloss: 0.0131665\n",
      "[428]\ttraining's multi_logloss: 0.013069\n",
      "[429]\ttraining's multi_logloss: 0.0129979\n",
      "[430]\ttraining's multi_logloss: 0.0128919\n",
      "[431]\ttraining's multi_logloss: 0.0128128\n",
      "[432]\ttraining's multi_logloss: 0.0127469\n",
      "[433]\ttraining's multi_logloss: 0.0126907\n",
      "[434]\ttraining's multi_logloss: 0.0126204\n",
      "[435]\ttraining's multi_logloss: 0.012557\n",
      "[436]\ttraining's multi_logloss: 0.0124953\n",
      "[437]\ttraining's multi_logloss: 0.012451\n",
      "[438]\ttraining's multi_logloss: 0.0123891\n",
      "[439]\ttraining's multi_logloss: 0.0123136\n",
      "[440]\ttraining's multi_logloss: 0.0122741\n",
      "[441]\ttraining's multi_logloss: 0.0121705\n",
      "[442]\ttraining's multi_logloss: 0.0120545\n",
      "[443]\ttraining's multi_logloss: 0.0119538\n",
      "[444]\ttraining's multi_logloss: 0.0118454\n",
      "[445]\ttraining's multi_logloss: 0.0117505\n",
      "[446]\ttraining's multi_logloss: 0.0116494\n",
      "[447]\ttraining's multi_logloss: 0.0115649\n",
      "[448]\ttraining's multi_logloss: 0.0114723\n",
      "[449]\ttraining's multi_logloss: 0.0113872\n",
      "[450]\ttraining's multi_logloss: 0.0113087\n",
      "[451]\ttraining's multi_logloss: 0.0112373\n",
      "[452]\ttraining's multi_logloss: 0.0111672\n",
      "[453]\ttraining's multi_logloss: 0.0111055\n",
      "[454]\ttraining's multi_logloss: 0.0110304\n",
      "[455]\ttraining's multi_logloss: 0.0109835\n",
      "[456]\ttraining's multi_logloss: 0.0109321\n",
      "[457]\ttraining's multi_logloss: 0.010871\n",
      "[458]\ttraining's multi_logloss: 0.0108047\n",
      "[459]\ttraining's multi_logloss: 0.0107663\n",
      "[460]\ttraining's multi_logloss: 0.0107273\n",
      "[461]\ttraining's multi_logloss: 0.0106386\n",
      "[462]\ttraining's multi_logloss: 0.0105628\n",
      "[463]\ttraining's multi_logloss: 0.0104849\n",
      "[464]\ttraining's multi_logloss: 0.010394\n",
      "[465]\ttraining's multi_logloss: 0.0103109\n",
      "[466]\ttraining's multi_logloss: 0.0102396\n",
      "[467]\ttraining's multi_logloss: 0.0101836\n",
      "[468]\ttraining's multi_logloss: 0.0101266\n",
      "[469]\ttraining's multi_logloss: 0.0100745\n",
      "[470]\ttraining's multi_logloss: 0.0100155\n",
      "[471]\ttraining's multi_logloss: 0.00997377\n",
      "[472]\ttraining's multi_logloss: 0.00992005\n",
      "[473]\ttraining's multi_logloss: 0.00988366\n",
      "[474]\ttraining's multi_logloss: 0.00984826\n",
      "[475]\ttraining's multi_logloss: 0.00979406\n",
      "[476]\ttraining's multi_logloss: 0.00976626\n",
      "[477]\ttraining's multi_logloss: 0.00972874\n",
      "[478]\ttraining's multi_logloss: 0.00968805\n",
      "[479]\ttraining's multi_logloss: 0.00964809\n",
      "[480]\ttraining's multi_logloss: 0.00962781\n",
      "[481]\ttraining's multi_logloss: 0.00953517\n",
      "[482]\ttraining's multi_logloss: 0.00946025\n",
      "[483]\ttraining's multi_logloss: 0.00939058\n",
      "[484]\ttraining's multi_logloss: 0.00932258\n",
      "[485]\ttraining's multi_logloss: 0.00926852\n",
      "[486]\ttraining's multi_logloss: 0.00920247\n",
      "[487]\ttraining's multi_logloss: 0.00915207\n",
      "[488]\ttraining's multi_logloss: 0.00909799\n",
      "[489]\ttraining's multi_logloss: 0.00905063\n",
      "[490]\ttraining's multi_logloss: 0.00900196\n",
      "[491]\ttraining's multi_logloss: 0.00895489\n",
      "[492]\ttraining's multi_logloss: 0.00892164\n",
      "[493]\ttraining's multi_logloss: 0.00888391\n",
      "[494]\ttraining's multi_logloss: 0.00884411\n",
      "[495]\ttraining's multi_logloss: 0.00881575\n",
      "[496]\ttraining's multi_logloss: 0.00878563\n",
      "[497]\ttraining's multi_logloss: 0.00875633\n",
      "[498]\ttraining's multi_logloss: 0.00872473\n",
      "[499]\ttraining's multi_logloss: 0.00870584\n",
      "[500]\ttraining's multi_logloss: 0.00869185\n",
      "[501]\ttraining's multi_logloss: 0.00853393\n",
      "[502]\ttraining's multi_logloss: 0.00830461\n",
      "[503]\ttraining's multi_logloss: 0.00814531\n",
      "[504]\ttraining's multi_logloss: 0.00806757\n",
      "[505]\ttraining's multi_logloss: 0.00800731\n",
      "[506]\ttraining's multi_logloss: 0.00795688\n",
      "[507]\ttraining's multi_logloss: 0.00790074\n",
      "[508]\ttraining's multi_logloss: 0.00786627\n",
      "[509]\ttraining's multi_logloss: 0.00781728\n",
      "[510]\ttraining's multi_logloss: 0.00778953\n",
      "[511]\ttraining's multi_logloss: 0.00774431\n",
      "[512]\ttraining's multi_logloss: 0.00770706\n",
      "[513]\ttraining's multi_logloss: 0.00767439\n",
      "[514]\ttraining's multi_logloss: 0.00763713\n",
      "[515]\ttraining's multi_logloss: 0.00759968\n",
      "[516]\ttraining's multi_logloss: 0.00755996\n",
      "[517]\ttraining's multi_logloss: 0.00753298\n",
      "[518]\ttraining's multi_logloss: 0.00750939\n",
      "[519]\ttraining's multi_logloss: 0.00749289\n",
      "[520]\ttraining's multi_logloss: 0.00747107\n",
      "[521]\ttraining's multi_logloss: 0.00742412\n",
      "[522]\ttraining's multi_logloss: 0.00738695\n",
      "[523]\ttraining's multi_logloss: 0.0073572\n",
      "[524]\ttraining's multi_logloss: 0.00730716\n",
      "[525]\ttraining's multi_logloss: 0.00726474\n",
      "[526]\ttraining's multi_logloss: 0.00723961\n",
      "[527]\ttraining's multi_logloss: 0.00720181\n",
      "[528]\ttraining's multi_logloss: 0.00715884\n",
      "[529]\ttraining's multi_logloss: 0.00712938\n",
      "[530]\ttraining's multi_logloss: 0.00709374\n",
      "[531]\ttraining's multi_logloss: 0.0070713\n",
      "[532]\ttraining's multi_logloss: 0.0070395\n",
      "[533]\ttraining's multi_logloss: 0.00701768\n",
      "[534]\ttraining's multi_logloss: 0.00698462\n",
      "[535]\ttraining's multi_logloss: 0.00695644\n",
      "[536]\ttraining's multi_logloss: 0.00694867\n",
      "[537]\ttraining's multi_logloss: 0.00692705\n",
      "[538]\ttraining's multi_logloss: 0.00691275\n",
      "[539]\ttraining's multi_logloss: 0.00689949\n",
      "[540]\ttraining's multi_logloss: 0.00687791\n",
      "[541]\ttraining's multi_logloss: 0.006838\n",
      "[542]\ttraining's multi_logloss: 0.00681174\n",
      "[543]\ttraining's multi_logloss: 0.00679139\n",
      "[544]\ttraining's multi_logloss: 0.00676539\n",
      "[545]\ttraining's multi_logloss: 0.00672877\n",
      "[546]\ttraining's multi_logloss: 0.00671803\n",
      "[547]\ttraining's multi_logloss: 0.00669113\n",
      "[548]\ttraining's multi_logloss: 0.00665891\n",
      "[549]\ttraining's multi_logloss: 0.00663327\n",
      "[550]\ttraining's multi_logloss: 0.00662026\n",
      "[551]\ttraining's multi_logloss: 0.00660522\n",
      "[552]\ttraining's multi_logloss: 0.00659504\n",
      "[553]\ttraining's multi_logloss: 0.00656945\n",
      "[554]\ttraining's multi_logloss: 0.00654285\n",
      "[555]\ttraining's multi_logloss: 0.00651923\n",
      "[556]\ttraining's multi_logloss: 0.00651368\n",
      "[557]\ttraining's multi_logloss: 0.00649162\n",
      "[558]\ttraining's multi_logloss: 0.00648294\n",
      "[559]\ttraining's multi_logloss: 0.00646468\n",
      "[560]\ttraining's multi_logloss: 0.006459\n",
      "[561]\ttraining's multi_logloss: 0.00636407\n",
      "[562]\ttraining's multi_logloss: 0.00631962\n",
      "[563]\ttraining's multi_logloss: 0.00628677\n",
      "[564]\ttraining's multi_logloss: 0.00625919\n",
      "[565]\ttraining's multi_logloss: 0.00622242\n",
      "[566]\ttraining's multi_logloss: 0.00618729\n",
      "[567]\ttraining's multi_logloss: 0.00616074\n",
      "[568]\ttraining's multi_logloss: 0.0061331\n",
      "[569]\ttraining's multi_logloss: 0.00608904\n",
      "[570]\ttraining's multi_logloss: 0.00607007\n",
      "[571]\ttraining's multi_logloss: 0.00604845\n",
      "[572]\ttraining's multi_logloss: 0.00603455\n",
      "[573]\ttraining's multi_logloss: 0.00602229\n",
      "[574]\ttraining's multi_logloss: 0.00600787\n",
      "[575]\ttraining's multi_logloss: 0.00599493\n",
      "[576]\ttraining's multi_logloss: 0.00598097\n",
      "[577]\ttraining's multi_logloss: 0.00596817\n",
      "[578]\ttraining's multi_logloss: 0.00595041\n",
      "[579]\ttraining's multi_logloss: 0.00593848\n",
      "[580]\ttraining's multi_logloss: 0.00592182\n",
      "[581]\ttraining's multi_logloss: 0.00591961\n",
      "[582]\ttraining's multi_logloss: 0.00589637\n",
      "[583]\ttraining's multi_logloss: 0.00588205\n",
      "[584]\ttraining's multi_logloss: 0.0058733\n",
      "[585]\ttraining's multi_logloss: 0.00585439\n",
      "[586]\ttraining's multi_logloss: 0.00583934\n",
      "[587]\ttraining's multi_logloss: 0.00582333\n",
      "[588]\ttraining's multi_logloss: 0.00581064\n",
      "[589]\ttraining's multi_logloss: 0.00579432\n",
      "[590]\ttraining's multi_logloss: 0.00578027\n",
      "[591]\ttraining's multi_logloss: 0.00576312\n",
      "[592]\ttraining's multi_logloss: 0.00575796\n",
      "[593]\ttraining's multi_logloss: 0.00575563\n",
      "[594]\ttraining's multi_logloss: 0.00575064\n",
      "[595]\ttraining's multi_logloss: 0.00574033\n",
      "[596]\ttraining's multi_logloss: 0.00574312\n",
      "[597]\ttraining's multi_logloss: 0.00574173\n",
      "[598]\ttraining's multi_logloss: 0.00572749\n",
      "[599]\ttraining's multi_logloss: 0.0057102\n",
      "[600]\ttraining's multi_logloss: 0.00570828\n",
      "[601]\ttraining's multi_logloss: 0.00544242\n",
      "[602]\ttraining's multi_logloss: 0.00539181\n",
      "[603]\ttraining's multi_logloss: 0.00536759\n",
      "[604]\ttraining's multi_logloss: 0.00532554\n",
      "[605]\ttraining's multi_logloss: 0.0053075\n",
      "[606]\ttraining's multi_logloss: 0.00528439\n",
      "[607]\ttraining's multi_logloss: 0.00526455\n",
      "[608]\ttraining's multi_logloss: 0.00523428\n",
      "[609]\ttraining's multi_logloss: 0.0052112\n",
      "[610]\ttraining's multi_logloss: 0.00519886\n",
      "[611]\ttraining's multi_logloss: 0.00519131\n",
      "[612]\ttraining's multi_logloss: 0.00517377\n",
      "[613]\ttraining's multi_logloss: 0.00515525\n",
      "[614]\ttraining's multi_logloss: 0.00513105\n",
      "[615]\ttraining's multi_logloss: 0.00510838\n",
      "[616]\ttraining's multi_logloss: 0.00509345\n",
      "[617]\ttraining's multi_logloss: 0.00507587\n",
      "[618]\ttraining's multi_logloss: 0.00505722\n",
      "[619]\ttraining's multi_logloss: 0.00504139\n",
      "[620]\ttraining's multi_logloss: 0.00502801\n",
      "[621]\ttraining's multi_logloss: 0.00494697\n",
      "[622]\ttraining's multi_logloss: 0.00490547\n",
      "[623]\ttraining's multi_logloss: 0.00487079\n",
      "[624]\ttraining's multi_logloss: 0.00484815\n",
      "[625]\ttraining's multi_logloss: 0.0048278\n",
      "[626]\ttraining's multi_logloss: 0.00480724\n",
      "[627]\ttraining's multi_logloss: 0.00478866\n",
      "[628]\ttraining's multi_logloss: 0.00477211\n",
      "[629]\ttraining's multi_logloss: 0.00475647\n",
      "[630]\ttraining's multi_logloss: 0.00474664\n",
      "[631]\ttraining's multi_logloss: 0.00473849\n",
      "[632]\ttraining's multi_logloss: 0.00473566\n",
      "[633]\ttraining's multi_logloss: 0.00471796\n",
      "[634]\ttraining's multi_logloss: 0.00470922\n",
      "[635]\ttraining's multi_logloss: 0.00470459\n",
      "[636]\ttraining's multi_logloss: 0.00468851\n",
      "[637]\ttraining's multi_logloss: 0.00468117\n",
      "[638]\ttraining's multi_logloss: 0.00467246\n",
      "[639]\ttraining's multi_logloss: 0.00466268\n",
      "[640]\ttraining's multi_logloss: 0.00466271\n",
      "[641]\ttraining's multi_logloss: 0.004642\n",
      "[642]\ttraining's multi_logloss: 0.0046243\n",
      "[643]\ttraining's multi_logloss: 0.00461525\n",
      "[644]\ttraining's multi_logloss: 0.00460232\n",
      "[645]\ttraining's multi_logloss: 0.00459028\n",
      "[646]\ttraining's multi_logloss: 0.00458073\n",
      "[647]\ttraining's multi_logloss: 0.00457845\n",
      "[648]\ttraining's multi_logloss: 0.00456278\n",
      "[649]\ttraining's multi_logloss: 0.00454832\n",
      "[650]\ttraining's multi_logloss: 0.00454296\n",
      "[651]\ttraining's multi_logloss: 0.00453369\n",
      "[652]\ttraining's multi_logloss: 0.00452067\n",
      "[653]\ttraining's multi_logloss: 0.00452169\n",
      "[654]\ttraining's multi_logloss: 0.0045219\n",
      "[655]\ttraining's multi_logloss: 0.00451711\n",
      "[656]\ttraining's multi_logloss: 0.00451277\n",
      "[657]\ttraining's multi_logloss: 0.00450739\n",
      "[658]\ttraining's multi_logloss: 0.0045155\n",
      "[659]\ttraining's multi_logloss: 0.00451489\n",
      "[660]\ttraining's multi_logloss: 0.0045198\n",
      "[661]\ttraining's multi_logloss: 0.00452385\n",
      "[662]\ttraining's multi_logloss: 0.00451135\n",
      "[663]\ttraining's multi_logloss: 0.00450102\n",
      "[664]\ttraining's multi_logloss: 0.00448349\n",
      "[665]\ttraining's multi_logloss: 0.00447795\n",
      "[666]\ttraining's multi_logloss: 0.00448498\n",
      "[667]\ttraining's multi_logloss: 0.00448372\n",
      "[668]\ttraining's multi_logloss: 0.00447936\n",
      "[669]\ttraining's multi_logloss: 0.00447167\n",
      "[670]\ttraining's multi_logloss: 0.00447521\n",
      "[671]\ttraining's multi_logloss: 0.00446771\n",
      "[672]\ttraining's multi_logloss: 0.00446526\n",
      "[673]\ttraining's multi_logloss: 0.0044651\n",
      "[674]\ttraining's multi_logloss: 0.00446282\n",
      "[675]\ttraining's multi_logloss: 0.00446256\n",
      "[676]\ttraining's multi_logloss: 0.00445686\n",
      "[677]\ttraining's multi_logloss: 0.00445283\n",
      "[678]\ttraining's multi_logloss: 0.00444062\n",
      "[679]\ttraining's multi_logloss: 0.00443936\n",
      "[680]\ttraining's multi_logloss: 0.00443344\n",
      "[681]\ttraining's multi_logloss: 0.00429251\n",
      "[682]\ttraining's multi_logloss: 0.0042226\n",
      "[683]\ttraining's multi_logloss: 0.00418283\n",
      "[684]\ttraining's multi_logloss: 0.00414642\n",
      "[685]\ttraining's multi_logloss: 0.00411337\n",
      "[686]\ttraining's multi_logloss: 0.00409588\n",
      "[687]\ttraining's multi_logloss: 0.00408354\n",
      "[688]\ttraining's multi_logloss: 0.00407227\n",
      "[689]\ttraining's multi_logloss: 0.00405343\n",
      "[690]\ttraining's multi_logloss: 0.00403169\n",
      "[691]\ttraining's multi_logloss: 0.0040124\n",
      "[692]\ttraining's multi_logloss: 0.00400045\n",
      "[693]\ttraining's multi_logloss: 0.0039861\n",
      "[694]\ttraining's multi_logloss: 0.00398365\n",
      "[695]\ttraining's multi_logloss: 0.00398037\n",
      "[696]\ttraining's multi_logloss: 0.00396523\n",
      "[697]\ttraining's multi_logloss: 0.00396149\n",
      "[698]\ttraining's multi_logloss: 0.00395633\n",
      "[699]\ttraining's multi_logloss: 0.00394665\n",
      "[700]\ttraining's multi_logloss: 0.0039464\n",
      "[701]\ttraining's multi_logloss: 0.00393704\n",
      "[702]\ttraining's multi_logloss: 0.00392892\n",
      "[703]\ttraining's multi_logloss: 0.0039167\n",
      "[704]\ttraining's multi_logloss: 0.00392232\n",
      "[705]\ttraining's multi_logloss: 0.0039214\n",
      "[706]\ttraining's multi_logloss: 0.00392314\n",
      "[707]\ttraining's multi_logloss: 0.00392057\n",
      "[708]\ttraining's multi_logloss: 0.00392267\n",
      "[709]\ttraining's multi_logloss: 0.00392052\n",
      "[710]\ttraining's multi_logloss: 0.00391221\n",
      "[711]\ttraining's multi_logloss: 0.00389883\n",
      "[712]\ttraining's multi_logloss: 0.00390103\n",
      "[713]\ttraining's multi_logloss: 0.00389915\n",
      "[714]\ttraining's multi_logloss: 0.00389381\n",
      "[715]\ttraining's multi_logloss: 0.00390049\n",
      "[716]\ttraining's multi_logloss: 0.00389023\n",
      "[717]\ttraining's multi_logloss: 0.00388403\n",
      "[718]\ttraining's multi_logloss: 0.00387585\n",
      "[719]\ttraining's multi_logloss: 0.00386928\n",
      "[720]\ttraining's multi_logloss: 0.00387689\n",
      "[721]\ttraining's multi_logloss: 0.00386546\n",
      "[722]\ttraining's multi_logloss: 0.00385115\n",
      "[723]\ttraining's multi_logloss: 0.00384865\n",
      "[724]\ttraining's multi_logloss: 0.00383816\n",
      "[725]\ttraining's multi_logloss: 0.0038373\n",
      "[726]\ttraining's multi_logloss: 0.00383801\n",
      "[727]\ttraining's multi_logloss: 0.003834\n",
      "[728]\ttraining's multi_logloss: 0.0038302\n",
      "[729]\ttraining's multi_logloss: 0.00382374\n",
      "[730]\ttraining's multi_logloss: 0.00382942\n",
      "[731]\ttraining's multi_logloss: 0.00382692\n",
      "[732]\ttraining's multi_logloss: 0.00382511\n",
      "[733]\ttraining's multi_logloss: 0.00382779\n",
      "[734]\ttraining's multi_logloss: 0.0038315\n",
      "[735]\ttraining's multi_logloss: 0.00383474\n",
      "[736]\ttraining's multi_logloss: 0.00383985\n",
      "[737]\ttraining's multi_logloss: 0.0038451\n",
      "[738]\ttraining's multi_logloss: 0.00384967\n",
      "[739]\ttraining's multi_logloss: 0.00384856\n",
      "[740]\ttraining's multi_logloss: 0.00384929\n",
      "[741]\ttraining's multi_logloss: 0.00384537\n",
      "[742]\ttraining's multi_logloss: 0.00383708\n",
      "[743]\ttraining's multi_logloss: 0.00382438\n",
      "[744]\ttraining's multi_logloss: 0.00381576\n",
      "[745]\ttraining's multi_logloss: 0.0038069\n",
      "[746]\ttraining's multi_logloss: 0.00380248\n",
      "[747]\ttraining's multi_logloss: 0.00380195\n",
      "[748]\ttraining's multi_logloss: 0.00379586\n",
      "[749]\ttraining's multi_logloss: 0.00380192\n",
      "[750]\ttraining's multi_logloss: 0.00378699\n",
      "[751]\ttraining's multi_logloss: 0.00377631\n",
      "[752]\ttraining's multi_logloss: 0.00377592\n",
      "[753]\ttraining's multi_logloss: 0.00376094\n",
      "[754]\ttraining's multi_logloss: 0.00375289\n",
      "[755]\ttraining's multi_logloss: 0.00375213\n",
      "[756]\ttraining's multi_logloss: 0.00374055\n",
      "[757]\ttraining's multi_logloss: 0.00373776\n",
      "[758]\ttraining's multi_logloss: 0.00374323\n",
      "[759]\ttraining's multi_logloss: 0.00373101\n",
      "[760]\ttraining's multi_logloss: 0.00373347\n",
      "[761]\ttraining's multi_logloss: 0.00373065\n",
      "[762]\ttraining's multi_logloss: 0.00373193\n",
      "[763]\ttraining's multi_logloss: 0.00372355\n",
      "[764]\ttraining's multi_logloss: 0.0037213\n",
      "[765]\ttraining's multi_logloss: 0.00371979\n",
      "[766]\ttraining's multi_logloss: 0.003726\n",
      "[767]\ttraining's multi_logloss: 0.00372605\n",
      "[768]\ttraining's multi_logloss: 0.00372898\n",
      "[769]\ttraining's multi_logloss: 0.0037311\n",
      "[770]\ttraining's multi_logloss: 0.0037314\n",
      "[771]\ttraining's multi_logloss: 0.00372643\n",
      "[772]\ttraining's multi_logloss: 0.00372664\n",
      "[773]\ttraining's multi_logloss: 0.00372555\n",
      "[774]\ttraining's multi_logloss: 0.0037289\n",
      "[775]\ttraining's multi_logloss: 0.00371819\n",
      "[776]\ttraining's multi_logloss: 0.00371664\n",
      "[777]\ttraining's multi_logloss: 0.00372068\n",
      "[778]\ttraining's multi_logloss: 0.00372394\n",
      "[779]\ttraining's multi_logloss: 0.0037325\n",
      "[780]\ttraining's multi_logloss: 0.00373325\n",
      "[781]\ttraining's multi_logloss: 0.00373623\n",
      "[782]\ttraining's multi_logloss: 0.00373978\n",
      "[783]\ttraining's multi_logloss: 0.00374458\n",
      "[784]\ttraining's multi_logloss: 0.00373039\n",
      "[785]\ttraining's multi_logloss: 0.00372743\n",
      "[786]\ttraining's multi_logloss: 0.00372838\n",
      "[787]\ttraining's multi_logloss: 0.00373187\n",
      "[788]\ttraining's multi_logloss: 0.00372907\n",
      "[789]\ttraining's multi_logloss: 0.00372651\n",
      "[790]\ttraining's multi_logloss: 0.00372047\n",
      "[791]\ttraining's multi_logloss: 0.00371804\n",
      "[792]\ttraining's multi_logloss: 0.0037211\n",
      "[793]\ttraining's multi_logloss: 0.00371743\n",
      "[794]\ttraining's multi_logloss: 0.00371384\n",
      "[795]\ttraining's multi_logloss: 0.00371913\n",
      "[796]\ttraining's multi_logloss: 0.00371937\n",
      "[797]\ttraining's multi_logloss: 0.00372146\n",
      "[798]\ttraining's multi_logloss: 0.00373035\n",
      "[799]\ttraining's multi_logloss: 0.00372981\n",
      "[800]\ttraining's multi_logloss: 0.00372793\n",
      "[801]\ttraining's multi_logloss: 0.00371113\n",
      "[802]\ttraining's multi_logloss: 0.00370695\n",
      "[803]\ttraining's multi_logloss: 0.00370686\n",
      "[804]\ttraining's multi_logloss: 0.003696\n",
      "[805]\ttraining's multi_logloss: 0.00370226\n",
      "[806]\ttraining's multi_logloss: 0.00370205\n",
      "[807]\ttraining's multi_logloss: 0.00370447\n",
      "[808]\ttraining's multi_logloss: 0.00369192\n",
      "[809]\ttraining's multi_logloss: 0.00369293\n",
      "[810]\ttraining's multi_logloss: 0.00368595\n",
      "[811]\ttraining's multi_logloss: 0.00368102\n",
      "[812]\ttraining's multi_logloss: 0.00368673\n",
      "[813]\ttraining's multi_logloss: 0.00368712\n",
      "[814]\ttraining's multi_logloss: 0.0036881\n",
      "[815]\ttraining's multi_logloss: 0.0036905\n",
      "[816]\ttraining's multi_logloss: 0.00369202\n",
      "[817]\ttraining's multi_logloss: 0.00369747\n",
      "[818]\ttraining's multi_logloss: 0.00370261\n",
      "[819]\ttraining's multi_logloss: 0.00369563\n",
      "[820]\ttraining's multi_logloss: 0.0036874\n",
      "[821]\ttraining's multi_logloss: 0.0036925\n",
      "[822]\ttraining's multi_logloss: 0.0036973\n",
      "[823]\ttraining's multi_logloss: 0.00369818\n",
      "[824]\ttraining's multi_logloss: 0.00369035\n",
      "[825]\ttraining's multi_logloss: 0.00369462\n",
      "[826]\ttraining's multi_logloss: 0.0037105\n",
      "[827]\ttraining's multi_logloss: 0.00370644\n",
      "[828]\ttraining's multi_logloss: 0.00370517\n",
      "[829]\ttraining's multi_logloss: 0.0037157\n",
      "[830]\ttraining's multi_logloss: 0.00372465\n",
      "[831]\ttraining's multi_logloss: 0.00371821\n",
      "[832]\ttraining's multi_logloss: 0.00372125\n",
      "[833]\ttraining's multi_logloss: 0.00373189\n",
      "[834]\ttraining's multi_logloss: 0.00374246\n",
      "[835]\ttraining's multi_logloss: 0.00373656\n",
      "[836]\ttraining's multi_logloss: 0.00373634\n",
      "[837]\ttraining's multi_logloss: 0.00373523\n",
      "[838]\ttraining's multi_logloss: 0.00373995\n",
      "[839]\ttraining's multi_logloss: 0.00374931\n",
      "[840]\ttraining's multi_logloss: 0.00375787\n",
      "[841]\ttraining's multi_logloss: 0.00376\n",
      "[842]\ttraining's multi_logloss: 0.00376005\n",
      "[843]\ttraining's multi_logloss: 0.00376377\n",
      "[844]\ttraining's multi_logloss: 0.00376424\n",
      "[845]\ttraining's multi_logloss: 0.00375417\n",
      "[846]\ttraining's multi_logloss: 0.00376165\n",
      "[847]\ttraining's multi_logloss: 0.00376331\n",
      "[848]\ttraining's multi_logloss: 0.0037723\n",
      "[849]\ttraining's multi_logloss: 0.00377309\n",
      "[850]\ttraining's multi_logloss: 0.00378011\n",
      "[851]\ttraining's multi_logloss: 0.00379113\n",
      "[852]\ttraining's multi_logloss: 0.00379526\n",
      "[853]\ttraining's multi_logloss: 0.00380284\n",
      "[854]\ttraining's multi_logloss: 0.00380632\n",
      "[855]\ttraining's multi_logloss: 0.00381144\n",
      "[856]\ttraining's multi_logloss: 0.00381283\n",
      "[857]\ttraining's multi_logloss: 0.00380936\n",
      "[858]\ttraining's multi_logloss: 0.00380665\n",
      "[859]\ttraining's multi_logloss: 0.00380951\n",
      "[860]\ttraining's multi_logloss: 0.00381192\n",
      "[861]\ttraining's multi_logloss: 0.00381289\n",
      "[862]\ttraining's multi_logloss: 0.00380991\n",
      "[863]\ttraining's multi_logloss: 0.0038129\n",
      "[864]\ttraining's multi_logloss: 0.00381576\n",
      "[865]\ttraining's multi_logloss: 0.00381632\n",
      "[866]\ttraining's multi_logloss: 0.0038297\n",
      "[867]\ttraining's multi_logloss: 0.00382693\n",
      "[868]\ttraining's multi_logloss: 0.00382696\n",
      "[869]\ttraining's multi_logloss: 0.00383595\n",
      "[870]\ttraining's multi_logloss: 0.00383803\n",
      "[871]\ttraining's multi_logloss: 0.00385095\n",
      "[872]\ttraining's multi_logloss: 0.00385713\n",
      "[873]\ttraining's multi_logloss: 0.00386652\n",
      "[874]\ttraining's multi_logloss: 0.00387478\n",
      "[875]\ttraining's multi_logloss: 0.00387147\n",
      "[876]\ttraining's multi_logloss: 0.00387188\n",
      "[877]\ttraining's multi_logloss: 0.0038763\n",
      "[878]\ttraining's multi_logloss: 0.0038687\n",
      "[879]\ttraining's multi_logloss: 0.0038625\n",
      "[880]\ttraining's multi_logloss: 0.00386532\n",
      "[881]\ttraining's multi_logloss: 0.00386708\n",
      "[882]\ttraining's multi_logloss: 0.00387044\n",
      "[883]\ttraining's multi_logloss: 0.00386492\n",
      "[884]\ttraining's multi_logloss: 0.00387154\n",
      "[885]\ttraining's multi_logloss: 0.00386463\n",
      "[886]\ttraining's multi_logloss: 0.00386293\n",
      "[887]\ttraining's multi_logloss: 0.00387133\n",
      "[888]\ttraining's multi_logloss: 0.00385826\n",
      "[889]\ttraining's multi_logloss: 0.00384991\n",
      "[890]\ttraining's multi_logloss: 0.0038477\n",
      "[891]\ttraining's multi_logloss: 0.00385273\n",
      "[892]\ttraining's multi_logloss: 0.00386383\n",
      "[893]\ttraining's multi_logloss: 0.00386376\n",
      "[894]\ttraining's multi_logloss: 0.00386287\n",
      "[895]\ttraining's multi_logloss: 0.00386793\n",
      "[896]\ttraining's multi_logloss: 0.00387406\n",
      "[897]\ttraining's multi_logloss: 0.00386895\n",
      "[898]\ttraining's multi_logloss: 0.00387513\n",
      "[899]\ttraining's multi_logloss: 0.00387517\n",
      "[900]\ttraining's multi_logloss: 0.00388093\n",
      "[901]\ttraining's multi_logloss: 0.00388966\n",
      "[902]\ttraining's multi_logloss: 0.00389308\n",
      "[903]\ttraining's multi_logloss: 0.00389199\n",
      "[904]\ttraining's multi_logloss: 0.00389511\n",
      "[905]\ttraining's multi_logloss: 0.00388869\n",
      "[906]\ttraining's multi_logloss: 0.00387936\n",
      "[907]\ttraining's multi_logloss: 0.00387707\n",
      "[908]\ttraining's multi_logloss: 0.00386774\n",
      "[909]\ttraining's multi_logloss: 0.00386945\n",
      "[910]\ttraining's multi_logloss: 0.00385997\n",
      "[911]\ttraining's multi_logloss: 0.00387446\n",
      "Early stopping, best iteration is:\n",
      "[811]\ttraining's multi_logloss: 0.00368102\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "##from sklearn.ensemble import RandomForestClassifier\n",
    "import lightgbm\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "## Grid Search Results of best parameters for the RandomForest Model. But since LightGBM is a Forest Ensamble\n",
    "## some of the optimized parameters would make good starting points.\n",
    "#'bootstrap': False,\n",
    "# 'max_depth': 100,\n",
    "# 'max_features': 'sqrt',\n",
    "# 'min_samples_leaf': 2,\n",
    "# 'min_samples_split': 2,\n",
    "# 'n_estimators': 2100\n",
    "\n",
    "#Random Forest using Grid Search best predicted params. Good starting point for the LightGBM model...\n",
    "##model = RandomForestClassifier(n_estimators = 2100, min_samples_split = 2, max_features = 'sqrt', max_depth = 100, \n",
    "##                               criterion = 'entropy', \n",
    "##                               bootstrap = False, random_state = 1)\n",
    "\n",
    "#put data in a Dataset for lightgbm to use.\n",
    "ds_train = lightgbm.Dataset(X_train_feature_reduction, y_train)\n",
    "ds_test  = lightgbm.Dataset(X_test_feature_reduction,  y_test)\n",
    "\n",
    "#***added max_bin\n",
    "#***org: num_leaves : 130\n",
    "parameters = {\n",
    "    'max_depth': 100,\n",
    "    'max_bin': 500,\n",
    "    'application': 'multiclass',\n",
    "    'objective': 'multiclass',\n",
    "    'num_class': 8,\n",
    "    'metric': 'multi_logloss',\n",
    "    'is_unbalance': 'false',\n",
    "    'boosting': 'gbdt',\n",
    "    'num_leaves': 250,\n",
    "    'feature_fraction': 0.5,\n",
    "    'bagging_fraction': 0.5,\n",
    "    'bagging_freq': 20,\n",
    "    'learning_rate': 0.05,\n",
    "    'verbose': 1\n",
    "}\n",
    "#Train or fit the model using training data    valid_sets=ds_test,\n",
    "model = lightgbm.train(parameters,\n",
    "                       ds_train,\n",
    "                       num_boost_round=5000,\n",
    "                       valid_sets=ds_train,\n",
    "                       early_stopping_rounds=100)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output for LightGBM will be a list of probabilities. Convert probabilities to binary prediction using argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.21438992e-14 2.78935899e-02 5.96652202e-01 ... 3.75152525e-01\n",
      "  2.39257931e-04 2.09720858e-05]\n",
      " [2.24413850e-15 2.94821677e-03 9.69781790e-01 ... 2.70737443e-02\n",
      "  1.88533851e-04 2.58798215e-06]\n",
      " [5.08021086e-15 5.55268807e-01 3.70340915e-01 ... 7.43255254e-02\n",
      "  4.51006440e-05 1.63941685e-05]\n",
      " ...\n",
      " [3.76101977e-16 2.05327263e-07 2.05404578e-05 ... 3.26520638e-04\n",
      "  5.48113640e-03 1.53745069e-07]\n",
      " [1.92096488e-16 8.93566521e-08 4.24518829e-06 ... 1.03249627e-04\n",
      "  3.62685708e-03 6.40624892e-08]\n",
      " [2.26388710e-16 1.12809292e-07 5.73114074e-06 ... 2.01589231e-04\n",
      "  3.81118777e-03 1.18787174e-07]]\n",
      "(565892, 8)\n",
      "565892\n"
     ]
    }
   ],
   "source": [
    "# Predict\n",
    "###final submission use this code: y_pred = model.predict(X_test[features_to_try]) \n",
    "###training to see accuracy use this code: y_pred = model.predict(X_test_feature_reduction)\n",
    "y_pred = model.predict(X_test[features_to_try])\n",
    "#what did we find:\n",
    "print(y_pred)\n",
    "print(y_pred.shape)\n",
    "\n",
    "#Take the highest probability using argmax() to find the best forest. argmax returns the index of the highest probability\n",
    "#The index corresponds to the 1-7 forest Cover_type\n",
    "predictions = []\n",
    "for i in y_pred:\n",
    "    predictions.append(np.argmax(i))\n",
    "\n",
    "print(len(predictions))\n",
    "\n",
    "# Calculate accuracy. Uncomment out the y_pred = model.predict(X_test_feature_reduction) from above...\n",
    "# then uncomment out the following 4 lines to test accuracy metrics\n",
    "#print(confusion_matrix(y_test,predictions))\n",
    "#print(classification_report(y_test,predictions))\n",
    "#acc1 = accuracy_score(y_test, predictions)\n",
    "#print('{} Accuracy: {:.3f}%'.format(model.__class__.__name__, acc1 * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Cover_Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>15121</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>15122</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>15123</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>15124</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>15125</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Id  Cover_Type\n",
       "0  15121           2\n",
       "1  15122           2\n",
       "2  15123           1\n",
       "3  15124           5\n",
       "4  15125           5"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = pd.read_csv(\"/kaggle/input/learn-together/test.csv\").Id\n",
    "my_submission = pd.DataFrame({'Id': idx, 'Cover_Type': predictions})\n",
    "# you could use any filename. We choose submission here\n",
    "my_submission.to_csv('submission.csv', index=False)\n",
    "my_submission.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
